{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0ce0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58f9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa84f1",
   "metadata": {},
   "source": [
    "#### Downloading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cbf6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"data/saved_models/fine_tuned_model_with_regularization\"\n",
    "tokenizer_path = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03852e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path, output_hidden_states=True).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.return_dict_in_generate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24766038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_model(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones(input_ids.shape, device=input_ids.device)\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "    output = model.generate(input_ids, max_new_tokens=1, attention_mask=attention_mask, pad_token_id=pad_token_id)\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9cc294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 + 213 = 264\n"
     ]
    }
   ],
   "source": [
    "print(prompt_model(\"51 + 213 =\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f4539",
   "metadata": {},
   "source": [
    "#### Fitting a Helix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0cad8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_from_segment(left, right, number):\n",
    "    data = []\n",
    "    for a in range(left, right + 1):\n",
    "        for b in range(left, right + 1):\n",
    "            c = a + b\n",
    "            prompt = f\"{a} + {b} = {c}\"\n",
    "            data.append((a, b, c, prompt))\n",
    "    return random.sample(data, number)\n",
    "\n",
    "def get_activation(prompt, layer_idx):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    return hidden_states[layer_idx][0, 0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc8b6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 1\n",
    "target_variance = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d78ec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:12<00:00, 164.09it/s]\n"
     ]
    }
   ],
   "source": [
    "H_all = []\n",
    "a_vals = []\n",
    "prompts_all = get_samples_from_segment(0, 249, 2000)\n",
    "for a, b, c, prompt in tqdm(prompts_all):\n",
    "    a_vals.append(a)\n",
    "    activation = get_activation(prompt, layer_idx)\n",
    "    H_all.append(activation)\n",
    "H_all = np.array(H_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48a2c996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA dim: 13\n",
      "H_pca.shape: (2000, 13)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=None)\n",
    "H_pca = pca.fit_transform(H_all)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "pca_dim = np.argmax(cumulative_variance >= target_variance) + 1\n",
    "H_pca = H_pca[:, :pca_dim]\n",
    "\n",
    "print(f\"PCA dim: {pca_dim}\")\n",
    "print(\"H_pca.shape:\", H_pca.shape)\n",
    "# in H_pca columns are orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd261d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [2, 5, 10, 100]\n",
    "def build_fourier_features(a):\n",
    "    features = [a]\n",
    "    for period in periods:\n",
    "        features.append(np.cos(2 * np.pi * a / period))\n",
    "        features.append(np.sin(2 * np.pi * a / period))\n",
    "    return np.array(features)\n",
    "\n",
    "def build_polynomial_features(a):\n",
    "    features = [1]\n",
    "    for i in range(1, 9):\n",
    "        features.append(a ** i)\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "B = np.array([build_fourier_features(a) for a in a_vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7feaf8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 9)\n",
      "R^2 в PCA-пространстве: 0.3993\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(B, H_pca)\n",
    "C_pca = reg.coef_\n",
    "r2 = reg.score(B, H_pca)\n",
    "print(C_pca.shape)\n",
    "print(f\"R^2 в PCA-пространстве: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5044a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "VT = pca.components_\n",
    "VT_k = pca.components_[:pca_dim, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d5063a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = VT_k.T @ C_pca\n",
    "H_helix = B @ C.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8f06e",
   "metadata": {},
   "source": [
    "#### Evaluating the Quality of the Helical Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4839286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patched_logit(a, b, a_bad, patched_activation, layer_idx=1):\n",
    "\n",
    "    prompt_bad = f\"{a_bad} + {b} =\"\n",
    "    inputs = tokenizer(prompt_bad, return_tensors='pt').to(device)\n",
    "    target_token = tokenizer.encode(f\"51 + 12 = {str(a + b)}\")[4]\n",
    "\n",
    "    def hook(module, inputs):\n",
    "        modified_inputs = (inputs[0].clone(),)\n",
    "        modified_inputs[0][0, 0] = torch.tensor(patched_activation, device=device)\n",
    "        return modified_inputs\n",
    "    \n",
    "    # https://github.com/huggingface/transformers/blob/v4.53.1/src/transformers/models/gpt2/modeling_gpt2.py#L1090\n",
    "    handle = model.transformer.h[layer_idx].register_forward_pre_hook(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    handle.remove()\n",
    "    \n",
    "    logits = F.softmax(outputs.logits[0, -1])\n",
    "    logit = logits[target_token].item()\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48214284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logit(a, b, a_bad):\n",
    "\n",
    "    prompt_bad = f\"{a_bad} + {b} =\"\n",
    "    inputs = tokenizer(prompt_bad, return_tensors='pt').to(device)\n",
    "    target_token = tokenizer.encode(f\"51 + 12 = {str(a + b)}\")[4]\n",
    "    # we need to do this to get right token\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = F.softmax(outputs.logits[0, -1])\n",
    "    logit = logits[target_token].item()\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1addbb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_67540/3328616041.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = F.softmax(outputs.logits[0, -1])\n",
      "/tmp/ipykernel_67540/3123055467.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = F.softmax(outputs.logits[0, -1])\n",
      "300it [00:07, 42.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean patching: 0.9341645966353733\n",
      "Helix patching: 0.3341626773095443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layer_idx = 1\n",
    "right_patching_improving = 0\n",
    "our_patching_improving = 0\n",
    "n = 300\n",
    "\n",
    "for i, (a, b, c, prompt) in tqdm(enumerate(prompts_all)):\n",
    "    if i >= n:\n",
    "        break\n",
    "    a_vals.append(a)\n",
    "    activation = get_activation(prompt, layer_idx)\n",
    "\n",
    "    right_patching = get_patched_logit(a, b, (b + 100) % 250, activation, layer_idx)\n",
    "    our_patching = get_patched_logit(a, b, (b + 100) % 250, H_helix[i], layer_idx)\n",
    "    without_patching = get_logit(a, b, (b + 100) % 250)\n",
    "    right_patching_improving += right_patching - without_patching\n",
    "    our_patching_improving += our_patching - without_patching\n",
    "\n",
    "print(\"Clean patching:\", right_patching_improving / n)\n",
    "print(\"Helix patching:\", our_patching_improving / n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc2e64d",
   "metadata": {},
   "source": [
    "- Clean patching: 0.94735\n",
    "- При target_variance = 0.995:\n",
    "    - Helix fit -> Helix patching: 0.3232\n",
    "    - Polynomial fit -> Polynomial patching: 0.0414\n",
    "\n",
    "- При target_variance = 0.95\n",
    "    - Helix patching: 0.3259\n",
    "\n",
    "- При target_variance = 0.75\n",
    "    - Helix patching: 0.1396\n",
    "\n",
    "- При target_variance = 0.5\n",
    "    - Helix patching: 0.0233\n",
    "\n",
    "\n",
    "- Используя gpt2 без SFT мы получим:\n",
    "    - Clean patching: -3.586612067010719e-05\n",
    "    - Helix patching: -0.0006099231596814067"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
